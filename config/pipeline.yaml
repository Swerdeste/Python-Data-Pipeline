# In the configuration file, you can :
# - define the pipeline by assembling several python jobs called "steps"
# - provide the parameters of the pipeline for each step
#
#

# ------------------ Step 1 ------------------ #
# Step 1: Merge the data

# The name of the step corresponds to the name of the python job to be executed.
# Here, the job is called "Merge_files.py"
- step-name: merge_files

  # - The list of csv files to be merged
  csv_files:

    # -- Ingestion of clinical_trials.csv
    - filename: clinical_trials.csv


      columns_to_rename:
        scientific_title: title
      read_csv:
        filepath_or_buffer: data/input/clinical_trials.csv
        encoding: utf-8

    # -- Ingestion of pubmed.csv
    - filename: pubmed.csv


      columns_to_rename:
        {}
      read_csv:
        filepath_or_buffer: data/input/pubmed.csv
        encoding: utf-8


  json_files:
    # -- Ingestion of pubmed.json
    - filename: pubmed.json


      columns_to_rename:
        {}
      read_json:
        filepath_or_buffer: data/input/pubmed.json
        encoding: "utf-8"

  filepath: src.jobs.generator.merge_files

# ------------------ Step 2 ------------------ #
# Step 2: Search for drugs appearences in the merged data

- step-name: find_drugs_appearances

  drugs:
    - name: drugs
      columns_to_rename:
        {}
      read_csv:
        filepath_or_buffer: data/input/drugs.csv
        encoding: utf-8

  filepath: src.jobs.processor.find_drugs_appearances


# ------------------ Step 3 ------------------ #
# Step 3: Create json ouput

- step-name: create_json

  output_name: data/output/output.json

  filepath: src.jobs.processor.create_json